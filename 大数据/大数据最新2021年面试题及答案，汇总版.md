# 大数据最新 2023 年面试题及答案，汇总版

### 全部答案，更新日期：2023 年 6 月 11 日，直接下载吧！

### 下载链接：[高清 172 份，累计 7701 页大厂面试题 PDF](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/index.md)

### [1、请用 java 实现非递归二分查询](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#1请用java实现非递归二分查询)

```
public class BinarySearchClass

{

public static int binary_search(int[] array, int value)

{

 int beginIndex = 0;// 低位下标

 int endIndex = array.length - 1;// 高位下标

 int midIndex = -1;

 while (beginIndex <= endIndex) {

     midIndex = beginIndex + (endIndex - beginIndex) / 2;//防止溢出

     if (value == array[midIndex]) {

         return midIndex;

     } else if (value < array[midIndex]) {

         endIndex = midIndex - 1;

     } else {

         beginIndex = midIndex + 1;

     }

 }

 return -1;

 //找到了，返回找到的数值的下标，没找到，返回-1

}

//start 提示：自动阅卷起始唯一标识，请勿删除或增加。

public static void main(String[] args)

{

 System.out.println("Start...");

 int[] myArray = new int[] { 1, 2, 3, 5, 6, 7, 8, 9 };

 System.out.println("查找数字8的下标：");

 System.out.println(binary_search(myArray, 8));

}

//end //提示：自动阅卷结束唯一标识，请勿删除或增加。

}
```

### [2、是客户端还是 Namenode 决定输入的分片？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#2是客户端还是namenode决定输入的分片)

这并不是客户端决定的，在配置文件中以及决定分片细则。

### [3、mapred.job.tracker 命令的作用？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#3mapredjobtracker命令的作用)

可以让你知道哪个节点是 Job Tracker。

### [4、全分布模式又有什么注意点？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#4全分布模式又有什么注意点)

全分布模式通常被用于生产环境，这里我们使用 N 台主机组成一个 Hadoop 集群，Hadoop 守护进程运行在每台主机之上。这里会存在 Namenode 运行的主机，Datanode 运行的主机，以及 task tracker 运行的主机。在分布式环境下，主节点和从节点会分开。

### [5、hive 跟 hbase 的区别](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#5hive-跟hbase的区别)

共同点都是用 hadoop 作为底层存储

区别：hive 是为了减少 mrjobs 编写工作的批处理系统，处理速度慢。hive 本身不存储数据和计算数据，依赖于 hadoop，纯逻辑表

hbase 是为了 hadoop 对实时操作的缺陷的项目，处理速度快，是物理表，提供一个超大的内存 hash 表，方便查询操作

如果全表扫描用 hive+hadoop

如果用索引查询与 hbase+hadoop

是处理数据库文件还是读取文本文件

先读取文本文件进行清洗，然后放入 hdfs，进行处理

或者直接读取 MySQL 中格式化数据

### [6、请列出正常工作的 hadoop 集群中 hadoop 都需要启动哪些进程，他们的作用分别是什么？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#6请列出正常工作的hadoop集群中hadoop都需要启动哪些进程他们的作用分别是什么)

NameNode: HDFS 的守护进程，负责记录文件是如何分割成数据块，以及这些数据块分别被存储到那些数据节点上，它的主要功能是对内存及 IO 进行集中管理

Secondary NameNode：辅助后台程序，与 NameNode 进行通信，以便定期保存 HDFS 元数据的快照。

DataNode：负责把 HDFS 数据块读写到本地的文件系统。

JobTracker：负责分配 task，并监控所有运行的 task。

TaskTracker：负责执行具体的 task，并与 JobTracker 进行交互。

### [7、KafkaUtils.createDstream 和 KafkaUtils.createDirectstream 区别](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#7kafkautilscreatedstream-和-kafkautilscreatedirectstream-区别)

使用一个 receiver 接收器接收数据，接收到的数据将会保存到 executor 中，然后通过 sparkStreaming 启动 job 来处理数据，默认不会丢失，可启动 WAL 日志，保存到 hdfs 上

spark.streaming.recever.writeAheadLog.enable=true 同时开启 StorageLevel.MeMORY_AND_DISK_SER_2

KafkaUtils.createDirectstream 方式，他定期从 Kafka 的分区中查询偏移量，再根据偏移量范围在每个 batch 里面处理数据

优点：简化并行 高效 恰好一次被消费

hbase

### [8、Kafka 与传统消息队列的区别](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#8kafka与传统消息队列的区别)

RabbitMQ 吞吐量稍差 Kafka，支持对消息可靠的传递，支持事务，不支持批量的操作，存储于内存或者磁盘

Kafka 遵从一般的 MQ 结构，producer，broker，consumer，以 consumer 为中心，消费的消费信息保存在客户端 consumer 上，consumer 根据消费的点，从 broker 批量 pull 数据；无消息确认机制

Kafka 具有搞得吞吐量，内部采用消息的批量处理，数据的存储和获取是本地磁盘顺序批量操作，消息处理的效率高

Kafka 的 broker 支持主备模式

Kafka 负载均衡 Zookeeper 方向

Kafka 采用 Zookeeper 进行管理，可以注册 topic 到 Zookeeper 上，通过 zoo 的协调机制，生产者保存对应 topic 的 broker 消息，可以随机或者轮询发送到 broker 上；并且生产者可以基于予以定义指定分片，消息发送到 broker 的某分片上

### [9、Master 文件是否提供了多个入口？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#9master文件是否提供了多个入口)

是的你可以拥有多个 Master 文件接口。

### [10、Spark 的数据本地性有哪几种？](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/大数据/大数据最新2021年面试题及答案，汇总版.md#10spark的数据本地性有哪几种)

Spark 中的数据本地性有三种：

**1、** PROCESS_LOCAL 是指读取缓存在本地节点的数据

**2、** NODE_LOCAL 是指读取本地节点硬盘数据

**3、** ANY 是指读取非本地节点数据

通常读取数据 PROCESS_LOCAL>NODE_LOCAL>ANY，尽量使数据以 PROCESS_LOCAL 或 NODE_LOCAL 方式读取。其中 PROCESS_LOCAL 还和 cache 有关，如果 RDD 经常用的话将该 RDD cache 到内存中，注意，由于 cache 是 lazy 的，所以必须通过一个 action 的触发，才能真正的将该 RDD cache 到内存中。

### 11、Spark 的 shuffle 过程？

### 12、fsimage 和 edit 的区别？

### 13、List 与 set 的区别

### 14、海量日志数据，提取出某日访问百度次数最多的那个 IP。

### 15、storm 特点

### 16、Hbase 宕机如何处理

### 17、Hbase 行键列族的概念，物理模型，表的设计原则？

### 18、Spark 为什么要持久化，一般什么场景下要进行 persist 操作？

### 19、有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M。返回频数最高的 100 个词。

### 20、SSH 中的注意点还包括？

### 21、存储特点

### 22、请简述 hadoop 怎样实现二级排序（就是对 key 和 value 双排序）

### 23、现场出问题测试 mapreduce 掌握情况和 hive 的 ql 语言掌握情况

### 24、hadoop 数据倾斜及解决办法

### 25、HDFS 读取文件的步骤

### 26、sqoop 在导入到 MySQL 中，如果不重复导入数据，如果数据存在问题，sqoop 如何处理？

### 27、什么是 udf

### 28、hadoop 的 TextInputFormat 作用是什么，如何自定义实现？

### 29、hbase 内部机制是什么

### 30、如何退出输入模式？

### 31、简述 hadoop spark storm hive 的特点及使用场景

### 32、combine 出现在哪个过程

## [全部答案，更新日期：2023 年 6 月 11 日，直接下载吧！](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/daan.md)

### 下载链接：[全部答案，整理好了](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/daan.md)

## [新增：高清 PDF：172 份，7701 页，最新整理](https://gitlab.gaorta.com/devteam/learning-journey/study-materials-collection/-/tree/master/docs/daan.md)
